{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#entregas-em-grupo","title":"\u2728Entregas em Grupo\u2728","text":""},{"location":"#sobre","title":"Sobre","text":"<ul> <li>Ana Carolina Frank</li> <li>Luzivania Bonfim</li> <li>Maria Luiza Oliveira</li> <li>Marcela de Martini</li> </ul> Informa\u00e7\u00f5es da Turma <ul> <li>Curso: Ci\u00eancia de Dados</li> <li>Disciplina: Machine Learning</li> <li>Semestre: 4\u00ba Semestre \u2014 2025.2</li> <li>Professor: Humberto Sandmann</li> </ul>"},{"location":"#entregas","title":"Entregas","text":"<p>Abaixo, voc\u00ea pode acompanhar o progresso das atividades:</p> <ul> <li> Projeto 1</li> <li> Projeto 2 </li> </ul>"},{"location":"projeto1/main/","title":"Projeto 1","text":""},{"location":"projeto1/main/#projeto-obesity-classification","title":"Projeto - Obesity Classification","text":"<p>Esse projeto tem como objetivo aplicar t\u00e9cnicas de Machine Learning, abordando \u00c1rvore de Decis\u00e3o, KNN e K-Means, para prever a presen\u00e7a de obesidade a partir de vari\u00e1veis f\u00edsicas.</p> <ul> <li> <p>Fonte: Obesity Classification</p> </li> <li> <p>Formato: 108 observa\u00e7\u00f5es x 7 colunas.</p> </li> <li> <p>Colunas: </p> <ul> <li>ID: Identificador</li> <li>Age: Idade do indiv\u00edduo</li> <li>Gender: Sexo (Male/Female)</li> <li>Height: Altura (Em CM)</li> <li>Weight: Peso (Em KG)</li> <li>BMI: Em portugu\u00eas, IMC. \u00cdndice de massa corporal.</li> <li>Label: Vari\u00e1vel alvo, com 4 classes:<ul> <li><code>Underweight</code> (0)</li> <li><code>Normal Weight</code> (1)</li> <li><code>Overweight</code> (2)</li> <li><code>Obese</code> (3)</li> </ul> </li> </ul> </li> </ul> <p>Observa\u00e7\u00e3o: A coluna <code>ID</code> foi removida das features, identificador n\u00e3o informativo.</p>"},{"location":"projeto1/main/#exploracao-de-dados","title":"Explora\u00e7\u00e3o de Dados","text":"<ul> <li>Distribui\u00e7\u00e3o de classes:</li> </ul> Classe Contagem Underweight 47 Normal Weight 29 Overweight 20 Obese 12 <p>Existe desbalanceamento pronunciado (Underweight \u2248 4\u00d7 Obese).  Isso exige cuidado: usar estratifica\u00e7\u00e3o no split, aplicar t\u00e9cnicas de oversampling (SMOTE) apenas no conjunto de treino ou utilizar <code>class_weight</code> em modelos que aceitam.</p>"},{"location":"projeto1/main/#verificacao-de-bmi","title":"Verifica\u00e7\u00e3o de <code>BMI</code>","text":"<p>Ao recalcular BMI a partir de Height e Weight (assumindo Height em cm, convertido para metros), encontramos inconsist\u00eancia significativa entre a coluna BMI fornecida e o BMI calculado:</p> <ul> <li>105 de 108 linhas (\u2248 97.22%) t\u00eam diferen\u00e7a absoluta |BMI - BMI_calc| &gt; 0.2.</li> </ul> <p>Confirmar a origem e unidade das colunas; recalcular BMI a partir de Height/Weight e usar o BMI recalculado (ou remover a coluna BMI original), pois valores inconsistentes podem introduzir ru\u00eddo severo.</p> Resultado ID Age Gender Height Weight BMI Label 80 60 Female 120 70 23.4 Normal Weight 11 18 Male 175 70 23.4 Normal Weight 5 45 Male 190 100 31.2 Obese 86 13 Male 175 25 10 Underweight 65 36 Male 190 75 24.2 Normal Weight 70 61 Female 120 75 25 Overweight 32 24 Female 160 55 21.2 Normal Weight 48 52 Female 130 75 25 Overweight 98 22 Male 180 20 8.3 Underweight 12 23 Female 160 50 20 Underweight"},{"location":"projeto1/main/#pre-processamento","title":"Pr\u00e9-Processamento","text":"<ul> <li>Remo\u00e7\u00e3o de <code>ID</code></li> <li>Mapeamento da target <code>Label</code> para inteiros:<ul> <li>Underweight: 0</li> <li>Normal: 1</li> <li>Overweight: 2</li> <li>Obese: 3</li> </ul> </li> <li><code>Gender</code> codificado com <code>LabelEncoder</code> (0/1).</li> <li>Convers\u00e3o de vari\u00e1veis categ\u00f3ricas com pd.get_dummies(..., drop_first=True) para modelos que exigem entradas num\u00e9ricas.</li> <li>Para KMeans: aplica\u00e7\u00e3o de <code>StandardScaler</code> antes do PCA/clusteriza\u00e7\u00e3o.</li> </ul> Resultado Age Gender Height Weight BMI Label 60 0 120 70 23.4 1 18 1 175 70 23.4 1 45 1 190 100 31.2 3 13 1 175 25 10 0 36 1 190 75 24.2 1 61 0 120 75 25 2 24 0 160 55 21.2 1 52 0 130 75 25 2 22 1 180 20 8.3 0 23 0 160 50 20 0"},{"location":"projeto1/main/#divisao-de-dados","title":"Divis\u00e3o de dados","text":"<p>Separar 70% para treino e 30% para teste: </p> <p><code>train_test_split(test_size=0.3, random_state=42, stratify=y).</code></p> <p>Treino (70%)</p> Label (num) Contagem 0 (Underweight) 33 1 (Normal Weight) 20 2 (Overweight) 14 3 (Obese) 8 <p>Teste (30%)</p> Label (num) Contagem 0 (Underweight) 14 1 (Normal Weight) 9 2 (Overweight) 6 3 (Obese) 4"},{"location":"projeto1/main/#modelagem-e-resultados","title":"Modelagem e resultados","text":"<p>Configura\u00e7\u00e3o geral: <code>random_state = 42</code>em opera\u00e7\u00f5es determin\u00edsticas; preservamos estratifica\u00e7\u00e3o no split para todas as avalia\u00e7\u00f5es.</p>"},{"location":"projeto1/main/#decision-tree","title":"Decision Tree","text":"<ul> <li>Resultado: Accuracy = 0.9697 (\u2248 96.97%).</li> </ul> Decision TreeCode <p>Accuracy: 0.88  2025-10-02T17:59:42.873571 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ </p> <pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\nfrom io import StringIO\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\n\n\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/bligui/Machine-Learning-Projetos/refs/heads/main/base/Obesity%20Classification.csv')\n\n#E\u00e9 a tabela que se refere ao id do individuo\ndf = df.drop(columns=['ID'])\n\n\ndf[\"Label\"] = df[\"Label\"].map({\n    \"Underweight\": 0,\n    \"Normal Weight\": 1,\n    \"Overweight\": 2,\n    \"Obese\": 3\n})\n\nlabel_encoder = LabelEncoder()  \ndf['Gender'] = label_encoder.fit_transform(df['Gender'])\n\nx = df.drop(columns=['Label'])\ny = df['Label']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42, stratify=y)\n\n# para Criar e treinar o modelo de \u00e1rvore de decis\u00e3o\nclassifier = tree.DecisionTreeClassifier(random_state=42)\nclassifier.fit(x_train, y_train)\n\nplt.figure(figsize=(12,10))\n\n# Avalia\u00e7\u00e3o o modelo, medindo a acuracia\naccuracy = classifier.score(x_test, y_test)\nprint(f\"Accuracy: {accuracy:.2f}\")\ntree.plot_tree(classifier)\n\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\")\nprint(buffer.getvalue())\n</code></pre> <p>Classification report (precision / recall / f1 / support):</p> <ul> <li> <p>Underweight: </p> <ul> <li>precision=1.00, </li> <li>recall=1.00, </li> <li>f1=1.00 (support=14)</li> </ul> </li> <li> <p>Normal: </p> <ul> <li>precision=0.90, </li> <li>recall=1.00, </li> <li>f1\u22480.947 (support=9)</li> </ul> </li> <li> <p>Overweight: </p> <ul> <li>precision=1.00, </li> <li>recall\u22480.833, </li> <li>f1\u22480.909 (support=6)</li> </ul> </li> <li> <p>Obese: </p> <ul> <li>precision=1.00,</li> <li>recall=1.00,</li> <li>f1=1.00 (support=4)</li> </ul> </li> </ul> <p>Interpreta\u00e7\u00e3o: desempenho muito alto no conjunto de teste, aten\u00e7\u00e3o ao overfitting, especialmente com \u00e1rvores n\u00e3o podadas em datasets pequenos. Recomenda-se valida\u00e7\u00e3o com CV e ajuste de hiperpar\u00e2metros (max_depth, min_samples_leaf, ccp_alpha).</p>"},{"location":"projeto1/main/#knn","title":"KNN","text":"<p>Resultado: Accuracy = 0.9394 (\u2248 93.94%). 5\u2011fold CV: m\u00e9dia \u2248 0.879654, desvio \u2248 0.022436.</p> KNNCode <p>Accuracy: 0.94  2025-10-02T17:59:56.802485 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/   Valida\u00e7\u00e3o Cruzada: 0.880 \u00b1 0.022 K=11 Accuracy: 0.879 Matriz de Confus\u00e3o: [[14  0  0  0]  [ 1  7  1  0]  [ 0  0  6  0]  [ 0  0  0  4]] </p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom io import StringIO\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndf = pd.read_csv('https://raw.githubusercontent.com/bligui/Machine-Learning-Projetos/refs/heads/main/base/Obesity%20Classification.csv')\ndf = df.drop(columns=['ID'])\n\ndf[\"Label\"] = df[\"Label\"].map({\n    \"Underweight\": 0,\n    \"Normal Weight\": 1,\n    \"Overweight\": 2,\n    \"Obese\": 3\n})\n\nlabel_encoder = LabelEncoder()  \ndf['Gender'] = label_encoder.fit_transform(df['Gender'])\n\nX = df[['Height', 'Weight']]\ny = df['Label']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Treinamento do KNN\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\n\n# Teste e valida\u00e7\u00e3o\npredictions = knn.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions):.2f}\")\n\n# Mapeamento CORRETO para todas as 4 classes\nlabels_map = {\n    0: \"Underweight\",\n    1: \"Normal Weight\", \n    2: \"Overweight\",\n    3: \"Obese\"\n}\ny_labels = y.map(labels_map)\n\n# Prepara\u00e7\u00e3o para o gr\u00e1fico\nh = 0.02\nx_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1\ny_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# Prevendo classe em cada ponto\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Gr\u00e1fico final com todas as 4 classes\npalette = {\n    \"Underweight\": 'blue',\n    \"Normal Weight\": 'green',\n    \"Overweight\": 'orange',\n    \"Obese\": 'red'\n}\n\nplt.figure(figsize=(10, 8))\nplt.contourf(xx, yy, Z, cmap=plt.cm.RdYlGn_r, alpha=0.3)\nsns.scatterplot(x=X.iloc[:, 0], y=X.iloc[:, 1], hue=y_labels, style=y_labels, palette=palette, s=100)\nplt.xlabel(\"Height\")\nplt.ylabel(\"Weight\")\nplt.title(\"KNN Decision Boundary (k=3) - Diagn\u00f3stico de Obesidade\")\nplt.legend(title=\"Diagn\u00f3stico de obesidade\")\n\n# Exibi\u00e7\u00e3o do gr\u00e1fico\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n\n\n\n\n\n\n# 1. Valida\u00e7\u00e3o Cruzada\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(knn, X, y, cv=5)\nprint(f\"Valida\u00e7\u00e3o Cruzada: {scores.mean():.3f} \u00b1 {scores.std():.3f}\")\n\n# 2. Comparar com k maior\nknn_k11 = KNeighborsClassifier(n_neighbors=11)\nknn_k11.fit(X_train, y_train)\nprint(f\"K=11 Accuracy: {accuracy_score(y_test, knn_k11.predict(X_test)):.3f}\")\n\n# 3. Matriz de Confus\u00e3o\nfrom sklearn.metrics import confusion_matrix\nprint(\"Matriz de Confus\u00e3o:\")\nprint(confusion_matrix(y_test, predictions))\n</code></pre> <p>Interpreta\u00e7\u00e3o: KNN com Height &amp; Weight separa bem as classes; performance sens\u00edvel a k e escalonamento. Recomenda-se testar StandardScaler e GridSearchCV para n_neighbors e weights.</p>"},{"location":"projeto1/main/#kmeans","title":"KMeans","text":"<p>PCA (2 componentes):</p> <ul> <li> <p>PC1: 0.5597427 (\u224855.97% da vari\u00e2ncia)</p> </li> <li> <p>PC2: 0.3135229 (\u224831.35% da vari\u00e2ncia)</p> </li> <li> <p>Soma \u2248 0.873266 (\u224887.33%)</p> </li> </ul> <p>Inertia (KMeans):</p> <ul> <li> <p>k = 4 \u2192 inertia = 96.192596</p> </li> <li> <p>k = 5 \u2192 inertia = 68.679993</p> </li> </ul> kmeanscode 2025-10-02T17:59:57.058539 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom io import StringIO\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport pandas as pd\n\n# Carregar dataset\ndf = pd.read_csv('https://raw.githubusercontent.com/bligui/Machine-Learning-Projetos/refs/heads/main/base/Obesity%20Classification.csv')\n\n# Features (remover a target e transformar vari\u00e1veis categ\u00f3ricas em dummies)\nX = pd.get_dummies(df.drop(columns=['Label']), drop_first=True)\n\n# Escalar dados\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Redu\u00e7\u00e3o de dimensionalidade PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# KMeans clustering\nkmeans = KMeans(n_clusters=5, init='k-means++', max_iter=100, random_state=42)\nlabels = kmeans.fit_predict(X_pca)\n\n# Adicionar clusters ao DataFrame original\ndf['Cluster'] = labels\n\n# Visualiza\u00e7\u00e3o dos clusters\nplt.figure(figsize=(10, 8))\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', s=50)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            c='red', marker='*', s=200, label='Centr\u00f3ides')\nplt.title('Clusters ap\u00f3s redu\u00e7\u00e3o de dimensionalidade (PCA)')\nplt.xlabel('Componente Principal 1')\nplt.ylabel('Componente Principal 2')\nplt.legend()\n\n# Salvar gr\u00e1fico no buffer (para exibir no Markdown)\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n\n# Caso queira ver localmente tamb\u00e9m, descomente:\n# plt.show()\n\n# Tabela de vari\u00e2ncia explicada\nvariancias = pca.explained_variance_ratio_\ntabela_variancia = pd.DataFrame({\n    'Componente Principal': [f'PC{i+1}' for i in range(len(variancias))],\n    'Vari\u00e2ncia Explicada': variancias,\n    'Vari\u00e2ncia Acumulada': np.cumsum(variancias)\n})\n\n# print(tabela_variancia.to_markdown(index=False))\n\n# Vari\u00e2ncia total explicada\n# print(\"\\nVari\u00e2ncia total explicada (2 componentes):\", np.sum(variancias))\n\n# Resultados do KMeans\n# print(\"\\nCentr\u00f3ides finais (no espa\u00e7o PCA):\", kmeans.cluster_centers_)\n# print(\"In\u00e9rcia (WCSS):\", kmeans.inertia_)\n</code></pre> <p>Interpreta\u00e7\u00e3o: os dois primeiros PCs explicam ~87% da vari\u00e2ncia, logo a proje\u00e7\u00e3o 2D \u00e9 representativa. k=5 apresenta menor inertia, mas a escolha final de k deve considerar silhouette score e interpreta\u00e7\u00e3o dos clusters em rela\u00e7\u00e3o \u00e0s classes reais.</p>"},{"location":"projeto1/main/#conclusao","title":"Conclus\u00e3o","text":"<ul> <li>Modelos testados (Decision Tree e KNN) apresentam desempenho elevado no conjunto de teste (\u224897% e \u224894%, respectivamente). Entretanto, o dataset \u00e9 pequeno e possui desbalanceamento e inconsist\u00eancia cr\u00edtica na coluna BMI.</li> </ul>"},{"location":"projeto2/main/","title":"Projeto 2","text":"<p>...</p>"}]}